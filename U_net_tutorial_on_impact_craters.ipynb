{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1_Cj6vJ_0-pMR3zPPlrmqgqCF_QdT041U",
      "authorship_tag": "ABX9TyNyOMVu6Z6pb5mvnJ+Qlyyq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williamlidberg/Unet-tutorial/blob/main/U_net_tutorial_on_impact_craters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to google colab"
      ],
      "metadata": {
        "id": "NppLzYcwYzMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cells\n",
        "A notebook is a list of cells. Cells contain either explanatory text or executable code and its output. Click a cell to select it."
      ],
      "metadata": {
        "id": "7HiFfWUAY1Xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code cells\n",
        "Below is a **code cell**. Once the toolbar button indicates CONNECTED, click in the cell to select it and execute the contents in the following ways:\n",
        "\n",
        "* Click the **Play icon** in the left gutter of the cell;\n",
        "* Type **Cmd/Ctrl+Enter** to run the cell in place;\n",
        "* Type **Shift+Enter** to run the cell and move focus to the next cell (adding one if none exists); or\n",
        "* Type **Alt+Enter** to run the cell and insert a new code cell immediately below it.\n",
        "\n",
        "There are additional options for running some or all cells in the **Runtime** menu.\n"
      ],
      "metadata": {
        "id": "bBpis6_FY28V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = 10\n",
        "a"
      ],
      "metadata": {
        "id": "DOKeOQKcYzhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with python\n",
        "Colaboratory is built on top of [Jupyter Notebook](https://jupyter.org/). Below are some examples of convenience functions provided."
      ],
      "metadata": {
        "id": "UGKhzb7qY7J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "print(\"Sleeping\")\n",
        "time.sleep(30) # sleep for a while; interrupt me!\n",
        "print(\"Done Sleeping\")"
      ],
      "metadata": {
        "id": "cBjnXfd7Y83S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding and moving cells\n",
        "You can add new cells by using the **+ CODE** and **+ TEXT** buttons that show when you hover between cells. These buttons are also in the toolbar above the notebook where they can be used to add a cell below the currently selected cell.\n",
        "\n",
        "You can move a cell by selecting it and clicking **Cell Up** or **Cell Down** in the top toolbar. \n",
        "\n",
        "Consecutive cells can be selected by \"lasso selection\" by dragging from outside one cell and through the group.  Non-adjacent cells can be selected concurrently by clicking one and then holding down Ctrl while clicking another.  Similarly, using Shift instead of Ctrl will select all intermediate cells."
      ],
      "metadata": {
        "id": "Ktwsjs5HZAal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep learning on the moon\n",
        "In this tutorial you will learn:\n",
        "\n",
        "\n",
        "1.   Google colab\n",
        "2.   Import python libraries\n",
        "3.   Run python code\n",
        "4.   Import data and plot images\n",
        "5.   Build a deep learning pipline\n",
        "6.   Build a basic Unet model\n",
        "7.   Train and test a Unet model\n",
        "8.   Adjust weights to classes\n"
      ],
      "metadata": {
        "id": "kml97TcLZF5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries\n",
        "Python libraries are like R packages. Before they can be used they need to be imported into the python environment."
      ],
      "metadata": {
        "id": "v1Rbm_5yZIyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os # For data handling\n",
        "import glob # For data handling\n",
        "import imageio # For images\n",
        "import cv2 # For images\n",
        "import numpy as np # For arrays\n",
        "import tensorflow.keras.backend as K # For deep learning\n",
        "import tensorflow as tf # For deep learning\n",
        "from skimage.transform import resize # For preprocessing\n",
        "import matplotlib.pyplot as plt # For plotting data\n",
        "from skimage.io import imread, imshow # for plots\n",
        "import random # for seeds\n",
        "random.seed(10) # To increase reproducability\n",
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JrOZl2ppZGQl",
        "outputId": "2bbeed9e-519d-4e60-9c83-01bfdd54e477"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import data"
      ],
      "metadata": {
        "id": "uFb2Ee2OhgsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!git clone https://github.com/williamlidberg/Unet-tutorial.git"
      ],
      "metadata": {
        "id": "KGd8uz_ThiP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect data\n",
        "The first stepp in any data analysis is always to inspect the data. The original lunar craters sort of look like hunting pits while the inverted data looks more like charcoal kilns. Start by checking the number of labeled files in the training data and testing data."
      ],
      "metadata": {
        "id": "COuJVWg1dSol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = glob.glob('/content/Unet-tutorial/craters_1000_samples/train/labels/*.png')\n",
        "print(len(labels),'images in the training data')\n",
        "\n",
        "labels = glob.glob('/content/Unet-tutorial/craters_1000_samples/test/labels/*.png')\n",
        "print(len(labels),'in the test data')"
      ],
      "metadata": {
        "id": "xB2qfAh7ZSqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot some samples of the images and labels. Note that the massive craters are not annotaded in this dataset."
      ],
      "metadata": {
        "id": "GD7JtJ0qdqnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# size of plot\n",
        "plt.rcParams['figure.figsize'] = [16, 16]\n",
        "\n",
        "# make lists of images\n",
        "labels = glob.glob('/content/Unet-tutorial/craters_1000_samples/train/labels/*.png')\n",
        "labels.sort()\n",
        "images = glob.glob('/content/Unet-tutorial/craters_1000_samples/test/images/*.png')\n",
        "images.sort()\n",
        "\n",
        "# select and plot images\n",
        "f, axarr = plt.subplots(2,2)\n",
        "axarr[0,0].set_title('Original data looks like hunting pits')\n",
        "axarr[0,0].imshow(imageio.imread(images[0]), cmap='Greys_r')\n",
        "axarr[1,0].set_title('Inverted data looks like charcoal kilns') \n",
        "axarr[1,0].imshow(imageio.imread(images[0]),cmap='Greys')\n",
        "\n",
        "axarr[0,1].set_title('Lunar Craters')\n",
        "axarr[0,1].imshow(imageio.imread(labels[0]))\n",
        "axarr[1,1].set_title('Lunar Craters')\n",
        "axarr[1,1].imshow(imageio.imread(labels[0]))\n",
        "\n"
      ],
      "metadata": {
        "id": "WqIe86h0Z9gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create training and testing data\n",
        "The images and labels are stored as images in the directories train and test. This script loads each image as a numpy array into a list and then converts this list of arrays into a [Tensorflow dataset]( https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n",
        "\n",
        "\n",
        "Each image is a 512x512 greyscale chips with only 1 band. (Normal color images have 3 bands, red, green and blue). The labels are binary where the value 0 indicates the background and 1 indicates a impact crater. The deep learning model expects values to range between 0 and 1 so the greyscale image will be normalized by deviding with the maximum value (255).\n",
        "\n",
        "To save some time all images will be resized to 256x256 for this tutorial.\n",
        "\n",
        "The \"%%time\" part is used to calculate how long time this script takes to run. This should take about 5 - 6 min."
      ],
      "metadata": {
        "id": "m4nFVDikeuk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "IMG_WIDTH = 256 # nr pixels\n",
        "IMG_HEIGHT = 256\n",
        "IMG_CHANNELS = 1 # a grey scale image only has one band for color.\n",
        "NUM_CLASSES = 1 # 0 = no crater and 1 = crater\n",
        "\n",
        "#### Training data ####\n",
        "TRAIN_PATH = '/content/Unet-tutorial/craters_1000_samples/train/'\n",
        "IMG_DIR = 'images'\n",
        "GT_DIR = 'labels'\n",
        "X_train = []\n",
        "Y_train = []\n",
        "\n",
        "# load from disk\n",
        "img_path = os.path.join(TRAIN_PATH, IMG_DIR)\n",
        "gt_path = os.path.join(TRAIN_PATH, GT_DIR)\n",
        "for image in (os.listdir(img_path)[0:300]):\n",
        "    img = imageio.imread(os.path.join(img_path, image))\n",
        "    img = img /255\n",
        "    img = resize(img, (IMG_WIDTH, IMG_HEIGHT,1), mode='constant', preserve_range=True)\n",
        "    \n",
        "    mask = imageio.imread(os.path.join(gt_path, image))\n",
        "    mask = resize(mask, (IMG_WIDTH, IMG_HEIGHT, 1), preserve_range=True, order=0).astype(int)\n",
        "    \n",
        "    X_train.append(img)\n",
        "    Y_train.append(mask)\n",
        "\n",
        "#### Test data ####\n",
        "TEST_PATH = '/content/Unet-tutorial/craters_1000_samples/test/'\n",
        "IMG_DIR = 'images'\n",
        "GT_DIR = 'labels'\n",
        "X_test = []\n",
        "Y_test = []\n",
        "\n",
        "# load from disk\n",
        "img_path = os.path.join(TEST_PATH, IMG_DIR)\n",
        "gt_path = os.path.join(TEST_PATH, GT_DIR)\n",
        "for image in (os.listdir(img_path)[0:300]):\n",
        "    img = imageio.imread(os.path.join(img_path, image))\n",
        "    img = img /255\n",
        "    img = resize(img, (IMG_WIDTH, IMG_HEIGHT,1), mode='constant', preserve_range=True)\n",
        "    \n",
        "    mask = imageio.imread(os.path.join(gt_path, image))\n",
        "    mask = resize(mask, (IMG_WIDTH, IMG_HEIGHT, 1), preserve_range=True, order=0).astype(int)\n",
        "    \n",
        "    X_test.append(img)\n",
        "    Y_test.append(mask)\n",
        "\n",
        "# convert list of numpy arrays into tensorflow dataset for further processing\n",
        "train_images = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
        "test_images = tf.data.Dataset.from_tensor_slices((X_test, Y_test))"
      ],
      "metadata": {
        "id": "aRo5WKiaeFo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build input pipelines\n",
        "[Batch size](https://medium.com/deep-learning-experiments/effect-of-batch-size-on-neural-net-training-c5ae8516e57) is a term used in machine learning and refers to the number of training examples utilized in one iteration.\n",
        "Buffer size is how many images that should be read into memory at the time. For a large number of images it is not possible to read the entire dataset into the system memory."
      ],
      "metadata": {
        "id": "909q_BDzgbFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training pipeline**"
      ],
      "metadata": {
        "id": "1Sylbq54hQVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4\n",
        "BUFFER_SIZE = 100\n",
        "train_batches = (train_images \n",
        "                    .cache() # cache data\n",
        "                    .shuffle(BUFFER_SIZE) # fill buffer, sample from it and replace with new items (buffer size > training set for perfect shuffling)\n",
        "                    .batch(BATCH_SIZE)  # set batch size\n",
        "                    .repeat()  # repeat dataset idefinetely\n",
        "                    .prefetch(buffer_size=100)) # prefetch 100 images to optimize runtime"
      ],
      "metadata": {
        "id": "pINwJIaXf5Aj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing pipeline**"
      ],
      "metadata": {
        "id": "kI7WUPJEhTGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup input pipeline\n",
        "BATCH_SIZE = 4\n",
        "BUFFER_SIZE = 100\n",
        "test_batches = (test_images \n",
        "                    .cache() # cache data\n",
        "                    .shuffle(BUFFER_SIZE) # fill buffer, sample from it and replace with new items (buffer size > training set for perfect shuffling)\n",
        "                    .batch(BATCH_SIZE)  # set batch size\n",
        "                    .repeat(1)  # repeat dataset idefinetely\n",
        "                    .prefetch(buffer_size=100)) # prefetch 100 images to optimize runtime"
      ],
      "metadata": {
        "id": "Eu_g0qswhVW0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define metrics to use for evaluation\n",
        "Accuracy is how many of the pixels are classified correctly by the model. This is not very usefull if most of the landscape consists of one class. In this case most of the lunar surface is infact not an impact crater. \n",
        "\n",
        "f1 score is a better option since it tries to take the inbalance of the data into account. f1 = 0 is very low and f1 = 1 is a perfect fit. Both recall and precision are required to calculate F1 score so these have to be defined as well.\n",
        "\n"
      ],
      "metadata": {
        "id": "zM_24YgsgdQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "metadata": {
        "id": "-G4M_u8kgelb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build U-net model\n",
        "[U-net](https://arxiv.org/abs/1505.04597) is a convolutional neural network that was developed for biomedical image segmentation. The network consists of a contracting path and an expansive path, which gives it the [u-shaped architecture](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/). The contracting path is a typical convolutional network that consists of repeated application of convolutions, each followed by a rectified linear unit (ReLU) and a max pooling operation. During the contraction, the spatial information is reduced while feature information is increased. The expansive pathway combines the feature and spatial information through a sequence of up-convolutions and concatenations with high-resolution features from the contracting path."
      ],
      "metadata": {
        "id": "G7SNFqh3gtEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the model\n",
        "inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "NUM_CLASSES = 1\n",
        "#Contraction path\n",
        "c1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
        "c1 = tf.keras.layers.Dropout(0.1)(c1) # to prevent overfitting\n",
        "c1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "c2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
        "c2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        " \n",
        "c3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
        "c3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        " \n",
        "c4 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
        "c4 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        " \n",
        "c5 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
        "c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
        "c5 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "\n",
        "#Expansive path \n",
        "u6 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "u6 = tf.keras.layers.concatenate([u6, c4])\n",
        "c6 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
        "c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
        "c6 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        " \n",
        "u7 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "u7 = tf.keras.layers.concatenate([u7, c3])\n",
        "c7 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
        "c7 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        " \n",
        "u8 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "u8 = tf.keras.layers.concatenate([u8, c2])\n",
        "c8 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
        "c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
        "c8 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
        " \n",
        "u9 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
        "c9 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
        "c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
        "c9 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
        " \n",
        "outputs = tf.keras.layers.Conv2D(NUM_CLASSES, (1, 1))(c9)\n",
        "\n",
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True), metrics=['acc',f1_m, recall_m])\n",
        "\n"
      ],
      "metadata": {
        "id": "aW3gKjhNgtzY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also print out a text summary of the model. Note that the first and last layers are both 256x256 just like our resized data."
      ],
      "metadata": {
        "id": "iuV21UdehnFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "ESZc73pvhkAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set weights for classes\n",
        "Most of the lunar surface is in fact not an impact crater so we can set weights to make the background less important for the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "-WL7-w1th7L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_sample_weights(image, label):\n",
        "    # The weights for each class, with the constraint that:\n",
        "    #     sum(class_weights) == 1.0\n",
        "    class_weights = tf.constant([0.05, 1]) # the first weight is for the background and the second for the craters.\n",
        "    class_weights = class_weights/tf.reduce_sum(class_weights)\n",
        "\n",
        "    # Create an image of `sample_weights` by using the label at each pixel as an \n",
        "    # index into the `class weights` .\n",
        "    sample_weights = tf.gather(class_weights, indices=tf.cast(label, tf.int32))\n",
        "\n",
        "    return image, label, sample_weights"
      ],
      "metadata": {
        "id": "iGnFqbuLhk6f"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the U-net"
      ],
      "metadata": {
        "id": "pXfbgChXh_rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.fit(train_batches.map(add_sample_weights), epochs=5, steps_per_epoch=100)"
      ],
      "metadata": {
        "id": "tNIC-Z2NiBPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the training history"
      ],
      "metadata": {
        "id": "DYEChOQTi6-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(result.history['f1_m'])\n",
        "plt.title('Model accuracy using f1 score')\n",
        "plt.ylabel('f1 score')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "K712GpdXi_Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model by applying it to the test images"
      ],
      "metadata": {
        "id": "vbrtqQbiiMdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_batches)"
      ],
      "metadata": {
        "id": "tdBAFswGiL4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model is not the best in the world and some of that might be due to the fact that only large craters are labeled."
      ],
      "metadata": {
        "id": "li57W8-mqPRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# size of plot\n",
        "plt.rcParams['figure.figsize'] = [16, 16]\n",
        "f, axarr = plt.subplots(1,3)\n",
        "\n",
        "#image \n",
        "testimage = X_test[0]\n",
        "orgimage = np.squeeze(testimage)\n",
        "\n",
        "# label\n",
        "testlabel = Y_test[0]\n",
        "testlabel = np.squeeze(testlabel)\n",
        "\n",
        "# prediction\n",
        "testimage = X_test[0]\n",
        "testimage = np.reshape(testimage,[1,256,256,1])\n",
        "prediction = model.predict(testimage)\n",
        "pred = np.squeeze(prediction)\n",
        "\n",
        "\n",
        "# Image\n",
        "axarr[0].set_title('Original image')\n",
        "axarr[0].imshow(orgimage, cmap='Greys_r')\n",
        "\n",
        "# Label\n",
        "axarr[1].set_title('Labeled craters')\n",
        "axarr[1].imshow(testlabel)\n",
        "\n",
        "# Prediction\n",
        "axarr[2].set_title('Predicted craters')\n",
        "axarr[2].imshow(pred>0.5)"
      ],
      "metadata": {
        "id": "7hj7VfKNlUiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot predicted craters on top of the original image"
      ],
      "metadata": {
        "id": "xzEmE5mglRZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = [16, 16]\n",
        "f, axarr = plt.subplots(1,2)\n",
        "\n",
        "axarr[0].set_title('Original image and labels')\n",
        "axarr[0].imshow(orgimage, cmap='Greys_r')\n",
        "axarr[0].imshow(testlabel, alpha=0.7)\n",
        "\n",
        "axarr[1].set_title('Original image and predictions')\n",
        "axarr[1].imshow(orgimage, cmap='Greys_r')\n",
        "axarr[1].imshow((pred>0.5), alpha=0.7)\n",
        "\n"
      ],
      "metadata": {
        "id": "6s4dBBNllX1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "\n",
        "\n",
        "1.   Does including more images produce better results?\n",
        "2.   Does an increased batch size improve the model?\n",
        "3.   Does the model preform better when training for more epochs?\n",
        "4.   How would the Unet be constructed if you wanted to train on the original larger images (512x512).\n",
        "5.   How much faster is training on a GPU compared to a CPU? (tip: use the %%time command)\n",
        "\n"
      ],
      "metadata": {
        "id": "MAeYnacimdn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Reading\n",
        "\n",
        "https://arxiv.org/abs/1505.04597\n",
        "\n",
        "https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d\n",
        "\n",
        "https://naokishibuya.medium.com/up-sampling-with-transposed-convolution-9ae4f2df52d0\n"
      ],
      "metadata": {
        "id": "S-EJ5Ip1pw_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Big thanks to Florian Westphal for making this demo work.\n",
        "\n",
        "## Contact\n",
        "William.lidberg@slu.se\n",
        "\n",
        "Florian.westphal@ju.se\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lEn_Gt_1p17R"
      }
    }
  ]
}