{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williamlidberg/Unet-tutorial/blob/main/U_net_tutorial_with_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detection of Hunting Pits usng Airborne Laser Scanning and Deep Learning\n",
        "This is an example from the [paper](https://www.tandfonline.com/doi/full/10.1080/00934690.2024.2364428) adapted to google colab. [Data](https://snd.se/en/catalogue/dataset/2023-188) and [code](https://github.com/williamlidberg/Detection-of-hunting-pits-using-airborne-laser-scanning-and-deep-learning) are openly avalible online."
      ],
      "metadata": {
        "id": "XNPn2lRX-88F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NppLzYcwYzMx"
      },
      "source": [
        "# Intro to google colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HiFfWUAY1Xa"
      },
      "source": [
        "# Cells\n",
        "A notebook is a list of cells. Cells contain either explanatory text or executable code and its output. Click a cell to select it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBpis6_FY28V"
      },
      "source": [
        "## Code cells\n",
        "Below is a **code cell**. Once the toolbar button indicates CONNECTED, click in the cell to select it and execute the contents in the following ways:\n",
        "\n",
        "* Click the **Play icon** in the left gutter of the cell;\n",
        "* Type **Cmd/Ctrl+Enter** to run the cell in place;\n",
        "* Type **Shift+Enter** to run the cell and move focus to the next cell (adding one if none exists); or\n",
        "* Type **Alt+Enter** to run the cell and insert a new code cell immediately below it.\n",
        "\n",
        "There are additional options for running some or all cells in the **Runtime** menu.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOKeOQKcYzhd"
      },
      "outputs": [],
      "source": [
        "a = 10\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGKhzb7qY7J4"
      },
      "source": [
        "# Working with python\n",
        "Colaboratory is built on top of [Jupyter Notebook](https://jupyter.org/). Below are some examples of convenience functions provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBjnXfd7Y83S"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "print(\"Sleeping\")\n",
        "time.sleep(10) # sleep for a while; interrupt me!\n",
        "print(\"Done Sleeping\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktwsjs5HZAal"
      },
      "source": [
        "## Adding and moving cells\n",
        "You can add new cells by using the **+ CODE** and **+ TEXT** buttons that show when you hover between cells. These buttons are also in the toolbar above the notebook where they can be used to add a cell below the currently selected cell.\n",
        "\n",
        "You can move a cell by selecting it and clicking **Cell Up** or **Cell Down** in the top toolbar.\n",
        "\n",
        "Consecutive cells can be selected by \"lasso selection\" by dragging from outside one cell and through the group.  Non-adjacent cells can be selected concurrently by clicking one and then holding down Ctrl while clicking another.  Similarly, using Shift instead of Ctrl will select all intermediate cells."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build your first neural network\n",
        "We will download some classified data from the internet to use as training data for your first model."
      ],
      "metadata": {
        "id": "rUCWkjNUjAFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Plot 25 sample images from the training set\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.xticks([]), plt.yticks([]), plt.grid(False)\n",
        "    plt.imshow(x_train[i])\n",
        "    plt.xlabel(class_names[y_train[i][0]])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0s5WivCFjHjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build a neural network"
      ],
      "metadata": {
        "id": "NbrXFrJ1jZuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the shape of the input images (flattened 32x32x3)\n",
        "image_shape = (32 * 32 * 3,)\n",
        "\n",
        "# Build the model\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=image_shape),                # Input layer\n",
        "    layers.Dense(32, activation='relu'),           # Hidden layer with 32 neurons\n",
        "    layers.Dense(10)                                # Output layer 10 classes: 'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "DbKH0wIijedC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ],
      "metadata": {
        "id": "BAKLzRQRjiay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the images and normalize pixel values\n",
        "x_train_flat = x_train.reshape(len(x_train), -1) / 255.0\n",
        "x_test_flat = x_test.reshape(len(x_test), -1) / 255.0\n",
        "\n",
        "# Train the model and save the history\n",
        "history = model.fit(x_train_flat, y_train, epochs=10,\n",
        "                    validation_data=(x_test_flat, y_test))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_acc = model.evaluate(x_test_flat, y_test, verbose=2)"
      ],
      "metadata": {
        "id": "ADSJmUatjkxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at the loss curves to see if the model overfits"
      ],
      "metadata": {
        "id": "L7fIkWFnmQ1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation loss and accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gKNWw1VimPbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep learning\n",
        "Now we will increase the models capacity to learn by adding more hidden layers with neurons. This is called deep learning."
      ],
      "metadata": {
        "id": "k-UmWLpQmXaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_shape = (32 * 32 * 3,)\n",
        "\n",
        "# Build the model\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=image_shape),                # Input layer\n",
        "    layers.Dense(128, activation='relu'),           # Hidden layer with 128 neurons\n",
        "    layers.Dense(64, activation='relu'),            # Hidden layer with 64 neurons\n",
        "    layers.Dense(32, activation='relu'),            # Hidden layer with 32 neurons\n",
        "    layers.Dense(10)                                # Output layer 10 classes: 'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model and save the history\n",
        "history = model.fit(x_train_flat, y_train, epochs=10,\n",
        "                    validation_data=(x_test_flat, y_test))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_acc = model.evaluate(x_test_flat, y_test, verbose=2)"
      ],
      "metadata": {
        "id": "vkVDru74mbLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did the model improve?"
      ],
      "metadata": {
        "id": "i8410s6Umjul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural networks\n",
        "So far we have used a one dimensional neural network that took flatted images as input. The next model will introduce convolutional layers. A convolutional layer is a neural network layer that automatically learns spatial patterns by applying filters to local regions of input data, such as images. Convolutional layers work by sliding learnable filters (kernels) over the input data to detect patterns like edges or textures, producing feature maps that highlight important spatial features. \\\n",
        "\\\n",
        "\\\n",
        "This is an example of a kernel sliding over an image.\n",
        "\n",
        "\n",
        "![Alt Text](https://maucher.home.hdm-stuttgart.de/Pics/gif/no_padding_no_strides.gif)\n",
        "\n",
        "\n",
        "\n",
        "We will start by replacing the flattend input from our previous model with the 2 d images with 3 channels. Next we will add a convolutional layer with the size 3 by 3 pixels. This layer will extract 32 feature layers that will be pooled and used as input for the fully connected neural network. We will also introduce max pooling and dropout. Max pooling is a way to select the maximum value for each region in the convolutional filter and dropout helps with overfitting.  "
      ],
      "metadata": {
        "id": "CJumEcgTmy68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# we will not flatten the images this time.\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "input_shape=(32, 32, 3) # the input shape is no longer flat.\n",
        "\n",
        "# Build the model\n",
        "model = models.Sequential([\n",
        "    layers.Input(input_shape),                     # Input for CIFAR-10 shape\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),  # Convolutional layer\n",
        "    layers.MaxPooling2D((2, 2)),                   # max pooling\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Flatten(),                              # flatten 2d data to 1 d\n",
        "    layers.Dense(128, activation='relu'),          # hidden layer with 128 neurons\n",
        "    layers.Dense(10)                               # Output layer\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model and save the history\n",
        "history = model.fit(x_train, y_train, epochs=10,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)"
      ],
      "metadata": {
        "id": "-F_Z97-0m15b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation loss and accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SU2Uknszm5X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ShCQD9XOjeB6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kml97TcLZF5j"
      },
      "source": [
        "# Deep learning with cultural remains\n",
        "In this tutorial you will learn:\n",
        "\n",
        "\n",
        "1.   Google colab\n",
        "2.   Import python libraries\n",
        "3.   Run python code\n",
        "6.   Build a Unet model\n",
        "7.   Train and test a Unet model\n",
        "8.   Transfer learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFb2Ee2OhgsM"
      },
      "source": [
        "## Import data\n",
        "These two cells downloads a dataset from williams google drive. The first cell is long and complicated due to googles shenanigans. The second cell unpacks the data into google colabs file tree."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "file_id = \"1EQKhlQFjg0mP7OfoT0NRURQZAgRAqBvj\"\n",
        "output = \"earth.zip\"\n",
        "gdown.download(\n",
        "    f\"https://drive.google.com/uc?export=download&confirm=pbef&id={file_id}\",\n",
        "    output\n",
        ")"
      ],
      "metadata": {
        "id": "tCRrPBUKAQ7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGd8uz_ThiP_"
      },
      "outputs": [],
      "source": [
        "!unzip /content/earth.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrOZl2ppZGQl"
      },
      "outputs": [],
      "source": [
        "import os # For data handling\n",
        "import glob # For data handling\n",
        "import numpy as np # For arrays\n",
        "import keras # for making models\n",
        "import visualkeras # for showing models\n",
        "import tensorflow.keras.backend as K # For deep learning\n",
        "import tensorflow as tf # For deep learning\n",
        "import matplotlib.pyplot as plt # For plotting data\n",
        "import random # for seeds\n",
        "random.seed(10) # To increase reproducability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COuJVWg1dSol"
      },
      "source": [
        "## Inspect data\n",
        "The first stepp in any data analysis is always to inspect the data. The data in this tutorial is stored as a tensorflow dataset. If you want to get into the details on how to create a training dataset you can check out the github linked to the research paper: https://github.com/williamlidberg/Detection-of-hunting-pits-using-airborne-laser-scanning-and-deep-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSkqPX4ZFkhC"
      },
      "outputs": [],
      "source": [
        "train_images = tf.data.Dataset.load('/content/earth/train/') # These will be used to train the model\n",
        "test_images = tf.data.Dataset.load('/content/earth/test/') # These will be used to test the model to get a estimate of the preformance.\n",
        "train_examples = list(train_images.as_numpy_iterator())\n",
        "print(len(train_images), 'images will be used to train the model.')\n",
        "print(len(test_images), 'images will be used to test the model.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfAW-XLjGZ0Q"
      },
      "source": [
        "Plot some examples from the training data to make sure it looks resonable. Hunting pits a tiny feaures and on the limit of what can be seen in this digital elevation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNlvEetyk4bw"
      },
      "outputs": [],
      "source": [
        "example1 = np.squeeze(train_examples[0]) # image number 0\n",
        "image1 = example1[0,:,:,]\n",
        "label1 = example1[1,:,:,]\n",
        "\n",
        "example2 = np.squeeze(train_examples[42]) # image number 42\n",
        "image2 = example2[0,:,:,]\n",
        "label2 = example2[1,:,:,]\n",
        "\n",
        "example3 = np.squeeze(train_examples[1000])  # image number 1000\n",
        "image3 = example3[0,:,:,]\n",
        "label3 = example3[1,:,:,]\n",
        "\n",
        "# select and plot images\n",
        "plt.rcParams['figure.figsize'] = [12, 12]\n",
        "f, axarr = plt.subplots(3,2)\n",
        "axarr[0,0].set_title('Hillshade')\n",
        "axarr[0,0].imshow(image1, cmap='Greys_r')\n",
        "axarr[0,1].set_title('Labeled hunting pit')\n",
        "axarr[0,1].imshow(label1,cmap='Greys')\n",
        "\n",
        "axarr[1,0].set_title('Hillshade')\n",
        "axarr[1,0].imshow(image2, cmap='Greys_r')\n",
        "axarr[1,1].set_title('Labeled hunting pit')\n",
        "axarr[1,1].imshow(label2,cmap='Greys')\n",
        "\n",
        "axarr[2,0].set_title('Hillshade')\n",
        "axarr[2,0].imshow(image3, cmap='Greys_r')\n",
        "axarr[2,1].set_title('Labeled hunting pit')\n",
        "axarr[2,1].imshow(label3,cmap='Greys')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Are points real hunting pits?"
      ],
      "metadata": {
        "id": "wpWhBKQ5ik2d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "909q_BDzgbFK"
      },
      "source": [
        "## Build input pipelines\n",
        "[Batch size](https://medium.com/deep-learning-experiments/effect-of-batch-size-on-neural-net-training-c5ae8516e57) is a term used in machine learning and refers to the number of training examples utilized in one iteration. Larger batch sizes is genreally better but GPU memory usually very limited.\n",
        "\n",
        "Buffer size is how many images that should be read into system memory (RAM) at the time. For a large number of images it is not possible to read the entire dataset into the system memory but having a small buffer ready speeds up the training process. Otherwise the images needs to be read from disk which is much slower."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sylbq54hQVf"
      },
      "source": [
        "**Training pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pINwJIaXf5Aj"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "BUFFER_SIZE = 32\n",
        "train_batches = (train_images\n",
        "                    .cache() # cache data\n",
        "                    .shuffle(BUFFER_SIZE) # fill buffer, sample from it and replace with new items (buffer size > training set for perfect shuffling)\n",
        "                    .batch(BATCH_SIZE)  # set batch size\n",
        "                    .repeat()  # repeat dataset idefinetely\n",
        "                    .prefetch(buffer_size=32)) # prefetch 100 images to optimize runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI7WUPJEhTGY"
      },
      "source": [
        "**Testing pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eu_g0qswhVW0"
      },
      "outputs": [],
      "source": [
        "# setup input pipeline\n",
        "BATCH_SIZE = 16\n",
        "BUFFER_SIZE = 32\n",
        "test_batches = (test_images\n",
        "                    .cache() # cache data\n",
        "                    .shuffle(BUFFER_SIZE) # fill buffer, sample from it and replace with new items (buffer size > training set for perfect shuffling)\n",
        "                    .batch(BATCH_SIZE)  # set batch size\n",
        "                    .repeat(1)  # repeat dataset idefinetely\n",
        "                    .prefetch(buffer_size=32)) # prefetch 100 images to optimize runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM_24YgsgdQd"
      },
      "source": [
        "## Define metrics to use for evaluation\n",
        "Accuracy is how many of the pixels are classified correctly by the model. This is not very usefull if most of the landscape consists of one class. In this case most of the forest are not hunting pits.\n",
        "\n",
        "f1 score is a better option since it tries to take the inbalance of the data into account. f1 = 0 is very low and f1 = 1 is a perfect fit. Both recall and precision are required to calculate F1 score so these have to be defined as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G4M_u8kgelb"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    y_true = K.cast(y_true, 'float32')  # Ensure both are float32\n",
        "    y_pred = K.cast(y_pred, 'float32')  # Ensure both are float32\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    y_true = K.cast(y_true, 'float32')  # Ensure both are float32\n",
        "    y_pred = K.cast(y_pred, 'float32')  # Ensure both are float32\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7SNFqh3gtEx"
      },
      "source": [
        "## Build U-net model\n",
        "[U-net](https://arxiv.org/abs/1505.04597) is a convolutional neural network that was developed for biomedical image segmentation. The network consists of a contracting path and an expansive path, which gives it the [u-shaped architecture](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/). The contracting path is a typical convolutional network that consists of repeated application of convolutions, each followed by a rectified linear unit (ReLU) and a max pooling operation. During the contraction, the spatial information is reduced while feature information is increased. The expansive pathway combines the feature and spatial information through a sequence of up-convolutions and concatenations with high-resolution features from the contracting path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW3gKjhNgtzY"
      },
      "outputs": [],
      "source": [
        "#Build the model\n",
        "IMG_WIDTH = 256 # image width in number of pixels\n",
        "IMG_HEIGHT = 256 # image height in number of pixels\n",
        "IMG_CHANNELS = 1 # a grey scale image only has one band. Normal images (RGB) have three channels.\n",
        "NUM_CLASSES = 1 # The images are labaled into 0 and 1 where 0 = no hunting pit and 1 = hunting pit\n",
        "\n",
        "inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "\n",
        "#Contraction path\n",
        "c1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs) # 32 is the number of feature maps and 3, 3 is the size of the convulutional filter.\n",
        "c1 = tf.keras.layers.Dropout(0.1)(c1) # dropouts combats overfitting\n",
        "c1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "c2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
        "c2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "c3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
        "c3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "c4 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
        "c4 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "\n",
        "c5 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
        "c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
        "c5 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "\n",
        "#Expansive path\n",
        "u6 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "u6 = tf.keras.layers.concatenate([u6, c4])\n",
        "c6 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
        "c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
        "c6 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        "\n",
        "u7 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "u7 = tf.keras.layers.concatenate([u7, c3])\n",
        "c7 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
        "c7 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "\n",
        "u8 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "u8 = tf.keras.layers.concatenate([u8, c2])\n",
        "c8 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
        "c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
        "c8 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
        "\n",
        "u9 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
        "c9 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
        "c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
        "c9 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
        "\n",
        "outputs = tf.keras.layers.Conv2D(NUM_CLASSES, (1, 1))(c9)\n",
        "\n",
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuV21UdehnFb"
      },
      "source": [
        "We can also print out a text summary of the model. Note that the first and last layers are both 256x256 just like our resized data. The 1 in the first layer represents the number of channels. A RGB image would have 3 channels but since this is a greyscale it only has 1."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That might be a bit dense to understand as a beginer. Another alternative is to make a figure of the model instead."
      ],
      "metadata": {
        "id": "w_a7gltQdVjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualkeras.layered_view(model, legend=True)"
      ],
      "metadata": {
        "id": "WK74UpGzdb_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXfbgChXh_rd"
      },
      "source": [
        "## Train the U-net\n",
        "Now we are ready to train the UNet. Epoch is one pass of the model across the training data. Training the model for more epochs takes longer time but gives more accurate results.\n",
        "\n",
        "The “steps per epoch” parameter is a value that defines the total number of steps taken before one epoch is finished and the next epoch starts during training. It is typically set to the total number of samples in your dataset divided by the batch size. 10 epochs takes around 5min to train on Colabs standard GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMyqUlh3rkqb"
      },
      "outputs": [],
      "source": [
        "steps = len(train_images)//BATCH_SIZE # len means number of images in the training data. Batch size was defined above.\n",
        "original_model = model\n",
        "original_model.compile(tf.keras.optimizers.Adam(learning_rate=0.001), loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True), metrics=['acc', f1_m, recall_m])\n",
        "result = original_model.fit(train_batches, epochs=5, steps_per_epoch=steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYEChOQTi6-Z"
      },
      "source": [
        "## Plot the training history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K712GpdXi_Hf"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.rcParams['figure.figsize'] = [8, 8]\n",
        "plt.plot(result.history['f1_m'])\n",
        "plt.title('Model accuracy using f1 score')\n",
        "plt.ylabel('f1 score')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.xlim(0,10)\n",
        "plt.ylim(0,1)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbrtqQbiiMdw"
      },
      "source": [
        "## Evaluate the model by applying it to the test images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdBAFswGiL4J"
      },
      "outputs": [],
      "source": [
        "results = original_model.evaluate(test_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li57W8-mqPRJ"
      },
      "source": [
        "The model seems to strugle to learn how to detect hunting pits. Probably due to a combination of a low number of training images and the fact that hunting pits are very small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yMfYOAhFGuO"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = [16, 16]\n",
        "f, axarr = plt.subplots(1,3)\n",
        "\n",
        "test_examples = list(test_images.as_numpy_iterator())\n",
        "test_example1 = np.squeeze(test_examples[1])\n",
        "original = test_example1[0,:,:,]\n",
        "label = test_example1[1,:,:,]\n",
        "\n",
        "testimage = np.reshape(original,[1,256,256,1])\n",
        "prediction = original_model.predict(testimage)\n",
        "pred = np.squeeze(prediction)\n",
        "\n",
        "# Image\n",
        "axarr[0].set_title('Original image')\n",
        "axarr[0].imshow(original, cmap='Greys_r')\n",
        "\n",
        "# Label\n",
        "axarr[1].set_title('Labeled pits')\n",
        "axarr[1].imshow(label, cmap='Greys_r')\n",
        "\n",
        "# Prediction\n",
        "axarr[2].set_title('Predicted pits')\n",
        "axarr[2].imshow(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can set pred>0.5 to get a binary prediction just like the label"
      ],
      "metadata": {
        "id": "eph9jGZKPIFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = [16, 16]\n",
        "f, axarr = plt.subplots(1,3)\n",
        "\n",
        "test_examples = list(test_images.as_numpy_iterator())\n",
        "test_example1 = np.squeeze(test_examples[1])\n",
        "original = test_example1[0,:,:,]\n",
        "label = test_example1[1,:,:,]\n",
        "\n",
        "testimage = np.reshape(original,[1,256,256,1])\n",
        "prediction = original_model.predict(testimage)\n",
        "pred = np.squeeze(prediction)\n",
        "\n",
        "# Image\n",
        "axarr[0].set_title('Original image')\n",
        "axarr[0].imshow(original, cmap='Greys_r')\n",
        "\n",
        "# Label\n",
        "axarr[1].set_title('Labeled pits')\n",
        "axarr[1].imshow(label, cmap='Greys_r')\n",
        "\n",
        "# Prediction\n",
        "axarr[2].set_title('Predicted pits')\n",
        "axarr[2].imshow(pred>0.5, cmap='Greys_r')"
      ],
      "metadata": {
        "id": "jt64hPRYPOWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osvDQHOtvltZ"
      },
      "source": [
        "# Transfer learning\n",
        "\n",
        "Transfer learning is a method where a model is first trained on different, but similair, data and then fine-tuned on the real data. This means that the model does not need to start from scratch when training on the real data.\n",
        "\n",
        "In our case we need a dataset with something similair to hunting pits. Luckley for us there is a place with plenty of pits that we can use. The moon!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBZYAw4ebpYh"
      },
      "source": [
        "## Download the trained moon model\n",
        "Start by cloning the github repository where the trained model is stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iz2OQDdTUua"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/williamlidberg/Detection-of-hunting-pits-using-airborne-laser-scanning-and-deep-learning.git /content/code/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icliBF8STm3T"
      },
      "source": [
        "Load the trained model and use it the moon"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/code/models/moon_hillshade.h5', by_name=True, skip_mismatch=True)  # Load weights into 'model'\n",
        "pre_trained_model = model\n",
        "pre_trained_model.compile(tf.keras.optimizers.Adam(learning_rate=0.001), loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True), metrics=['acc', f1_m, recall_m])"
      ],
      "metadata": {
        "id": "Yh498q0HAIA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_EXdwVgYH-R"
      },
      "source": [
        "# Train the lunar model on data of hunting pits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "steps = len(train_images) // BATCH_SIZE\n",
        "result_pretrained = pre_trained_model.fit(train_batches, validation_data=test_batches, epochs=10, steps_per_epoch=steps)  # Use 'model' for training"
      ],
      "metadata": {
        "id": "Q2bf7pMhJbwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.rcParams['figure.figsize'] = [8, 8]\n",
        "plt.plot(result_pretrained.history['f1_m'])\n",
        "plt.title('Model accuracy using f1 score')\n",
        "plt.ylabel('f1 score')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.xlim(0,10)\n",
        "plt.ylim(0,1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EhcVaaw_BV5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You could train the model for longer and get even better results but at some point the curve will level out. Test the model on the test data and compare the results to the original model."
      ],
      "metadata": {
        "id": "PExClhx6R2eV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG0v4xUJcFYo"
      },
      "outputs": [],
      "source": [
        "pre_trained_model.evaluate(test_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can take the new model and try it on a test image."
      ],
      "metadata": {
        "id": "dwFWJLOCSJ1s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a99ItVQict9O"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = [16, 16]\n",
        "f, axarr = plt.subplots(1,4)\n",
        "\n",
        "test_examples = list(test_images.as_numpy_iterator())\n",
        "test_example1 = np.squeeze(test_examples[1])\n",
        "original = test_example1[0,:,:,]\n",
        "label = test_example1[1,:,:,]\n",
        "\n",
        "testimage = np.reshape(original,[1,256,256,1])\n",
        "# original model\n",
        "prediction = original_model.predict(testimage)\n",
        "pred = np.squeeze(prediction)\n",
        "\n",
        "#pre trained model\n",
        "prediction_pretrained = pre_trained_model.predict(testimage)\n",
        "pred_pretrained = np.squeeze(prediction_pretrained)\n",
        "\n",
        "# Image\n",
        "axarr[0].set_title('Original image')\n",
        "axarr[0].imshow(original, cmap='Greys_r')\n",
        "\n",
        "# Label\n",
        "axarr[1].set_title('Labeled pits')\n",
        "axarr[1].imshow(label, cmap='Greys_r')\n",
        "\n",
        "# Prediction from original model\n",
        "axarr[2].set_title('Predicted pits original model')\n",
        "axarr[2].imshow(pred)#>0.5, cmap='Greys_r')\n",
        "\n",
        "# Prediction\n",
        "axarr[3].set_title('Predicted pits pre-trained model')\n",
        "axarr[3].imshow(pred_pretrained)#>0.5, cmap='Greys_r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAeYnacimdn2"
      },
      "source": [
        "## Questions\n",
        "\n",
        "\n",
        "\n",
        "1.   Will including more images produce better results?\n",
        "2.   Will an increased batch size improve the model?\n",
        "3.   Will the model preform better when training for more epochs?\n",
        "4.   Does training the original model for twice as lone result in the same accuracy as the pre-trained model?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-EJ5Ip1pw_2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Reading\n",
        "\n",
        "https://arxiv.org/abs/1505.04597\n",
        "\n",
        "https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d\n",
        "\n",
        "https://naokishibuya.medium.com/up-sampling-with-transposed-convolution-9ae4f2df52d0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEn_Gt_1p17R"
      },
      "source": [
        "## Contact\n",
        "William.lidberg@slu.se\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1_Cj6vJ_0-pMR3zPPlrmqgqCF_QdT041U",
      "authorship_tag": "ABX9TyNsqCwHt27dzPdJ99DPwRpN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}